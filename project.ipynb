{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Machine Learning Project"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Overview\n## Introduction"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Machine learning is an emerging technology with a growing relevance in medicine. With the wealth of medical data that's becoming available it is now possible to use machine learning to assist in diagnosing disease, however these predictions need to be very accurate as a patient's health is of the greatest concern. It is therefore necessary to rigorously test any machine learning models in this domain to determine which, if any, performs best.\n\nThe aim of this project is to determine which of 4 machine learning models best predicts the presence of heart disease using a dataset obtained from the UCI Machine Learning repository. The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/statlog+%28heart%29). "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Aims and Methodology"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this study we will examine 4 different models to determine which performs best. The 4 models are k-nearest neighbors, decision trees, naive bayes, and random forests. \n\nAfter preliminary data preparation and exploration, we partition our dataset into training and testing sets. \n\nOnce our data is ready, prior to testing, we do some optimisation: \n* we perform feature selection to determine an optimal subset of descriptive features.\n* we then perform hyperparameter tuning using grid search on the training dataset to optimise the parameters of our models.\n\nWhen our models are optimised we perform cross-validation on each of the optimised models using the testing data to obtain a measure of their performance. These scores are then analysed using paired t-tests to determine their significance.\n\nWe conclude with a discussion."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Table Of Contents:\n* [Data Description](#data-desc)\n* [Data Preparation](#dataprep)\n* [Predictive Modelling](#pred-mod)\n    * [Cross-Validation](#cross-val)\n    * [Feature Selection](#feature-sel)\n    * [Hyperparameter Tuning](#hyper-tun)\n* [Model Fitting and Performance Evaluation](#model-fit)\n* [Discussion](#discussion)\n    * [Confusion Matrix, precision, recall and F1-score](#conf-mat)\n    * [ROC Curve](#roc-curve)\n* [Conclusion](#conclusion)\n* [References](#refs)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"data-desc\"></a>\n# Data Description"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our data consists of 270 instances with 13 descriptive features, and a target feature having two classes which indicate the presence or absence of heart disease. Hence, this is a binary classification problem. As is, our dataset contains no missing values.\n\nThe following explanation of the descriptive features is extracted from the dataset description on the [UCI website](https://archive.ics.uci.edu/ml/datasets/statlog+%28heart%29) (note that the last descriptive feature 'thal' does not come with any information as to its meaning):\n* age - **numerical**\n* sex - **binary**\n* chest pain type - **nominal**\n* resting blood pressure - **numerical**\n* serum cholestoral in mg/dl - **numerical**      \n* fasting blood sugar > 120 mg/dl - **binary**\n* resting electrocardiographic results - **nominal**\n* maximum heart rate achieved - **numerical**\n* exercise induced angina - **binary**\n* oldpeak = ST depression induced by exercise relative to rest - **numerical**\n* the slope of the peak exercise ST segment - **ordinal**\n* number of major vessels (0-3) colored by flourosopy - **numerical**    \n* thal (3 = normal; 6 = fixed defect; 7 = reversible defect) - **nominal**\n\nOne thing that we observed in this dataset is that each of the nominal features has been integer encoded, which is a miselading representation of the data and will be addressed in the processing stage. \n\nTime to import our dataset as well as any modules we will be using, and do some preliminary configuration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for plotting\n#!pip install --upgrade pip\n#!pip install --upgrade altair\n#!pip install vega vega_datasets",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import altair as alt\nalt.renderers.enable('notebook')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "RendererRegistry.enable('notebook')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n\n# ignore python warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set random state for reproducibility\nrandom_seed = 999\n\n# display all columns\npd.set_option('display.max_columns', None) \n\n# handle figure numbers\nfignr = list(range(1,100))\n\n# read in and configure data with column names\n# column names are abbreviated below for space\ncol_names = ['age', 'sex', 'cpt', 'rb', 'sc', 'fbs', 'rer', 'mha', 'eia', 'old', 'sope', 'nomv', 'thal', 'target'] \nfull_data = pd.read_csv('heart.csv',names=col_names,header=None)\n\n# separate target feature\ntarget = full_data['target']\ndata = full_data.copy()\ndata.drop(columns=['target'],inplace=True)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "full_data.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cpt</th>\n      <th>rb</th>\n      <th>sc</th>\n      <th>fbs</th>\n      <th>rer</th>\n      <th>mha</th>\n      <th>eia</th>\n      <th>old</th>\n      <th>sope</th>\n      <th>nomv</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>130.0</td>\n      <td>322.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>109.0</td>\n      <td>0.0</td>\n      <td>2.4</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>67.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>115.0</td>\n      <td>564.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>160.0</td>\n      <td>0.0</td>\n      <td>1.6</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>124.0</td>\n      <td>261.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>128.0</td>\n      <td>263.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>105.0</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>74.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>120.0</td>\n      <td>269.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>121.0</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "    age  sex  cpt     rb     sc  fbs  rer    mha  eia  old  sope  nomv  thal  \\\n0  70.0  1.0  4.0  130.0  322.0  0.0  2.0  109.0  0.0  2.4   2.0   3.0   3.0   \n1  67.0  0.0  3.0  115.0  564.0  0.0  2.0  160.0  0.0  1.6   2.0   0.0   7.0   \n2  57.0  1.0  2.0  124.0  261.0  0.0  0.0  141.0  0.0  0.3   1.0   0.0   7.0   \n3  64.0  1.0  4.0  128.0  263.0  0.0  0.0  105.0  1.0  0.2   2.0   1.0   7.0   \n4  74.0  0.0  2.0  120.0  269.0  0.0  2.0  121.0  1.0  0.2   1.0   1.0   3.0   \n\n   target  \n0       2  \n1       1  \n2       2  \n3       1  \n4       1  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have our dataset imported and our target feature separated we can have a brief look at our dataset using python."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"----------\")\nprint(f\"Shape of the dataset is {data.shape} \\n\")\nprint(\"----------\")\nprint(f\"Each of the descriptive features have the following types:\\n{data.dtypes}\\n\")\nprint(\"----------\")\nprint(f\"Each of the descriptive features have the following number of unique values:\\n{data.nunique()}\\n\")\nprint(\"----------\")\nprint(f\"The dataset contains no missing values:\\n{data.isna().sum()}\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"dataprep\"></a>\n# Data Preparation\n\nWhile our dataset comes to us fairly clean, there is still much data preparation to do before we can get to the business of predictive modelling.\n\nFirst of all, we must integer encode the target feature. It currently holds the values (1) for the absence of heart disease, and (2) for the presence of heart disease. Since the presence is our postive feature, we wish to map this to (1) and the absence to (0)."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# firstly we integer encode target feature \nprint(\"Target before encoding: \",np.unique(target,return_counts=True))\nencoded_target = np.where(target==1,0,1)\nprint(\"Target after encoding: \",np.unique(encoded_target,return_counts=True))",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Target before encoding:  (array([1, 2]), array([150, 120]))\nTarget after encoding:  (array([0, 1]), array([150, 120]))\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now turn to our nominal features (chest pain type, resting electrocardiographic results, and thal) which have been integer encoded. As integer encoding assumes an ordering, we consider it bad practice to integer encode nominal features, so we will undo this encoding to repace it with a one-hot encoding scheme."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# output the value counts for variable 'cpt'\nprint(f\"Before processing the variable \\\"cpt\\\" has the following value counts:\\n{data['cpt'].value_counts()}\\n\")\n\n# we then map each of them to the following categorical levels\ncpt_mappings = {1.0 : 'cpt1', 2.0 : 'cpt2', 3.0 : 'cpt3', 4.0 : 'cpt4'}\ndata['cpt'].replace(cpt_mappings,inplace=True)\n\n# output the result\nprint(f\"After processing the variable \\\"cpt\\\" has the following value counts:\\n{data['cpt'].value_counts()}\\n\")\n\n# output the value counts for variable 'rer'\nprint(f\"Before processing the variable \\\"rer\\\" has the following value counts:\\n{data['rer'].value_counts()}\\n\")\n\n# we then map each of them to the following categorical levels\nrer_mappings = {0.0 : 'rer0', 1.0 : 'rer1', 2.0 : 'rer2'}\ndata['rer'].replace(rer_mappings,inplace=True)\n\n# output the result\nprint(f\"After processing the variable \\\"rer\\\" has the following value counts:\\n{data['rer'].value_counts()}\\n\")\n\n# output the value counts for variable 'thal'\nprint(f\"Before processing the variable \\\"thal\\\" has the following value counts:\\n{data['thal'].value_counts()}\\n\")\n\n# we then map each of them to the following categorical levels\nthal_mappings = {3.0 : 'thal3', 6.0 : 'thal6', 7.0 : 'thal7'}\ndata['thal'].replace(thal_mappings,inplace=True)\n\n# output the result\nprint(f\"After processing the variable \\\"thal\\\" has the following value counts:\\n{data['thal'].value_counts()}\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we've undone the integer encoding, we're ready to apply one-hot encoding to these nominal variables."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# one-hot encode\ndata = pd.get_dummies(data)\n# display transformed data\nprint(f\"The shape of the one-hot encoded data is {data.shape}\\n\")\ndata.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that our descriptive features are encoded correctly we can scale our data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\n\n# copy the processed data\ndata_copy = data.copy()\n\n# perform scaling\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(data)\ndata = scaler.fit_transform(data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our data has now been processed and scaled and we're ready to move to the next section."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data Exploration & Visualisation"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's take a closer look at our dataset and examine some of our descriptive features. Note that we will be referring to a copy of the original data with full column names for ease of examination, and to not interfere with data prepared for modelling. This data is shown below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "col_names = ['age', 'sex', 'chest pain type', 'resting bp', 'serum cholestoral', \n'fasting blood sugar', 'resting ecg results', 'max hr achieved', 'exercise induced angina', \n'oldpeak', 'slope of peak exercise', 'no of major vessels', 'thal', 'target']\ndata2 = full_data.copy()\ndata2.columns = col_names\n\n# Encoding target (1,2) which means absence/presence of heart disease to (0,1) according to discussion above\ndf_temp = data2.assign(encoded_target=(data2['target'] + 1) % 2)\ndata2['target'] = df_temp['encoded_target']\ndata2.head()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>chest pain type</th>\n      <th>resting bp</th>\n      <th>serum cholestoral</th>\n      <th>fasting blood sugar</th>\n      <th>resting ecg results</th>\n      <th>max hr achieved</th>\n      <th>exercise induced angina</th>\n      <th>oldpeak</th>\n      <th>slope of peak exercise</th>\n      <th>no of major vessels</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>130.0</td>\n      <td>322.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>109.0</td>\n      <td>0.0</td>\n      <td>2.4</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>67.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>115.0</td>\n      <td>564.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>160.0</td>\n      <td>0.0</td>\n      <td>1.6</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>124.0</td>\n      <td>261.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>128.0</td>\n      <td>263.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>105.0</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>74.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>120.0</td>\n      <td>269.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>121.0</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "    age  sex  chest pain type  resting bp  serum cholestoral  \\\n0  70.0  1.0              4.0       130.0              322.0   \n1  67.0  0.0              3.0       115.0              564.0   \n2  57.0  1.0              2.0       124.0              261.0   \n3  64.0  1.0              4.0       128.0              263.0   \n4  74.0  0.0              2.0       120.0              269.0   \n\n   fasting blood sugar  resting ecg results  max hr achieved  \\\n0                  0.0                  2.0            109.0   \n1                  0.0                  2.0            160.0   \n2                  0.0                  0.0            141.0   \n3                  0.0                  0.0            105.0   \n4                  0.0                  2.0            121.0   \n\n   exercise induced angina  oldpeak  slope of peak exercise  \\\n0                      0.0      2.4                     2.0   \n1                      0.0      1.6                     2.0   \n2                      0.0      0.3                     1.0   \n3                      1.0      0.2                     2.0   \n4                      1.0      0.2                     1.0   \n\n   no of major vessels  thal  target  \n0                  3.0   3.0       1  \n1                  0.0   7.0       0  \n2                  0.0   7.0       1  \n3                  1.0   7.0       0  \n4                  1.0   3.0       0  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Below we show the distribution of values for gender as well as for that of chest pain type.\n\nAnalysing the plots below reveals some imbalances:\n\n- There is roughly double the amount of gender 1.0 than 0.0 \n- The chest pain type 1.0 has about 20 patients which is quite a small sample"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nax = data2['sex'].value_counts().plot(kind = 'bar')\nplt.title('Figure {}: Bar chart of gender counts'.format(fignr.pop(0)), fontsize = 15)\nplt.xlabel('Gender')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# the distribution of each of the chest pain types\nax = data2['chest pain type'].value_counts().plot(kind = 'bar')\nplt.title('Figure {}: Bar chart of the amount for each Chest Pain Type'.format(fignr.pop(0)), fontsize = 15)\nplt.xlabel('Chest Pain Type')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The boxplot below shows that the median age is around 55 years with an interquartile range of around 48 to 62 years. It is not skewed in any directions which shows that the data is normal distribution. #TODO: Change descprition depending... \n#TODO: just because it isn't skewed doesnt mean it's normal!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.boxplot(data2['age']).set_title('Figure {}: Box plot of age'.format(fignr.pop(0)), fontsize = 15)\nplt.show();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The distribution can be shown in a histogram in figure 3."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# TODO: REMOVE because boxplot visualisation is easier to analyse\n# # get a histogram of age with kernel density estimate\n# TODO this might be good though!\nsns.distplot(data2['age'], kde = True).set_title('Figure {}: Histogram of Age'.format(fignr.pop(0)), fontsize = 15)\nplt.show();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following histogram shows the probability (y-axis) of each resting blood pressure (x-axis) with the line showing the kernel density. The boxplot below shows that the median of the resting bloodpressure is around 130 which is a little bit above the desired blodpressure of 120. #TODO: back this up"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "feature = 'resting bp'\nsns.distplot(data2[feature], kde = True).set_title('Figure {}: Histogram of resting bloodpressure'.format(fignr.pop(0)), fontsize = 15)\nplt.show();\nsns.boxplot(data2[feature]).set_title('Figure {}: Box Plot of resting bloodpressure'.format(fignr.pop(0)), fontsize = 15)\nplt.show();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The figure below show that the distribution of ages within each gender is similar, but with a slight difference where gender '1.0' is on average younger than gender '0.0'. The minimum age is about 5 years younger. #TODO: ??"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Creating a boxplot\nsns.boxplot(data2['sex'], data2['max hr achieved']);\nplt.title('Figure {}: Boxplot of max heartrate achieved  by Gender'.format(fignr.pop(0)), fontsize = 15)\nplt.show();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Below we will combine some of our numerical features to determine their relationship to our target feature. Our numerical features are: age, resting bp, serum cholestoral number, max heart rate achieved, and old peak.\n\nFrom the boxplots below we infer that patients diagnosed with heart disease in general have:\n\n- A higher age\n- A higher resting blood pessure\n- Approximately the same serum cholestoral\n- A significantly lower achieved maximum heart rate\n- A significantly higher oldpeak value (ST depression induced by exercise relative to rest)"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def show_2feature_boxplot(data, f1_nom=None, f2_num=None):\n    sns.boxplot(data[f1_nom], data2[f2_num]);\n    plt.title('Figure {}: Boxplot of {} by {}'.format(fignr.pop(0), f2_num, f1_nom), fontsize = 15)\n    plt.show();\n\n# show_2feature_boxplot(data2, f1_nom='sex', f2_num='resting bp')\n# TODO: these need to be separated into separate frames!\nshow_2feature_boxplot(data2, f1_nom='target', f2_num='age')\nshow_2feature_boxplot(data2, f1_nom='target', f2_num='resting bp')\nshow_2feature_boxplot(data2, f1_nom='target', f2_num='serum cholestoral')\nshow_2feature_boxplot(data2, f1_nom='target', f2_num='max hr achieved')\nshow_2feature_boxplot(data2, f1_nom='target', f2_num='oldpeak')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here we use multivariate plots to analyse the relationship between our target feature and some of our categorical features, which include: sex/gender, chest pain type, fasting blood sugar, resting ecg results, exercise induced angina, slope of peak exercise, no of major vessels and thal. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Below is the combination of age (numerical), chest pain type (nominal, 4 levels) and sex (nominal, 2 levels) combined. The result shows that difference of age with the different chest pain types varies a lot more with gender 0.0 than for gender 1.0"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Show 3 features, where one is numerical (f1) and two is nominal (f2, f3) with only one chosen level on the latter feature (f3)\ndef show_3feature_boxplot(data, f1, f2, f3, f3_level):\n    levels = data[f2].unique().tolist()\n    \n    # Make a mask of f2 and f3 with f3's chosen level\n    mask = (data[f2].isin(levels)) & (data[f3].isin([f3_level]))\n    mask_level = data[mask]\n    sns.boxplot(mask_level[f2], mask_level[f1]);\n    plt.title('Figure {}: Boxplot of the {} {} and {} by {}'.format(fignr.pop(0), f3, f3_level, f2, f1), fontsize = 15)\n    plt.show();\n\n# Showing two plots for each gender\nshow_3feature_boxplot(data2, 'age', 'chest pain type', 'sex', '0.0')\nshow_3feature_boxplot(data2, 'age', 'chest pain type', 'sex', '1.0')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To get a better view of the features combined a multivariate plot of three features can be shown using the method below. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Show 3 features, where one is numerical (f1) and two is nominal (f2, f3). Notice that f2 levels needs to be max 2 like Gender\ndef show_4feature_boxplot(data, f1, f2, f3):\n    # Getting the index of those who have completed their Bachelors or HS graduate\n    levels = data2[f2].unique().tolist()\n    f2_mask = data2[f2].isin(levels)\n\n    # Getting the index of those who are male and Not_in_family or a Husband\n    levels_f3 = data2[f3].unique().tolist()\n    f3_level1_mask = (data2[f3].isin(levels_f3)) & (data2[f2].isin([levels[0]]))\n    f3_level2_mask = (data2[f3].isin(levels_f3)) & (data2[f2].isin([levels[1]]))\n\n    # Selecting the rows of those who are Not_in_family, husband or wife and \n    # have completed either a Bachelors or just graduated high school\n    f2_f3_mask = data2[(f2_mask & f3_level2_mask) | (f2_mask & f3_level1_mask)]\n\n    # Creating the boxplot\n    sns.boxplot(f2_f3_mask[f3], f2_f3_mask[f1], \n                hue = f2_f3_mask[f2])\n    plt.title('Figure {}: Boxplot of {} broken down by {} and {}'.format(fignr.pop(0), f1, f2, f3), fontsize = 15)\n    plt.show();\n    \nshow_4feature_boxplot(data2, 'age', 'target', 'chest pain type')\nshow_4feature_boxplot(data2, 'max hr achieved', 'target', 'thal')\nshow_4feature_boxplot(data2, 'resting bp', 'target', 'sex')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The plots above \\# TODO: draw conclusions"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To easier display a combination of numerical features a scatter plot has been used. They can present two features that are numerical and one nominal feature, and these are shown below with some different combinations."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Show a scatterplot of 3 features where f1 is nominal and f2 and f3 is numerical\ndef show_scatterplot(data, f1, f2, f3):\n    # Getting the index of those in feature 1\n    f1_levels = data2[f1].unique().tolist()\n    mask = data2[f1].isin(f1_levels)\n\n    # creating a dataframe of those in feature1\n    f1_df = data2[mask]\n\n    # creating a scatterplot\n    sns.scatterplot(f1_df[f2], f1_df[f3], hue = f1_df[f1])\n    plt.title('Figure {}: Scatterplot of {} by {} coloured by each {}'.format(fignr.pop(0), f2, f3, f1), fontsize = 15);\n    plt.legend(loc = 'upper right')\n    plt.show();\n\n    # TODO: put each of these in a separate cell and talk about them\nshow_scatterplot(data2, 'target', 'oldpeak','max hr achieved')\nshow_scatterplot(data2, 'target', 'resting bp', 'max hr achieved')\nshow_scatterplot(data2, 'target', 'age', 'serum cholestoral')\nshow_scatterplot(data2, 'target', 'resting bp', 'serum cholestoral')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The conclusions from the scatterplots are: \\# TODO: CONCLUSION\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"pred-mod\"></a>\n# Predictive Modelling"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we are well acquainted with our data and it is had been prepared, we can get to the business of predictive modelling. \n\nWe first select a cross-validation method to use throughout our testing, then moving to optimisation with feature selecting and hyperparameter tuning, before final testing the performance of our optimised models."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"cross-val\"></a>\n## Cross-Validation\nTo assess the performance of our models we will be using repeated stratified 5-fold cross-validation with 3 repetitions. Stratified cross-validation was selected to ensure the proportion of positive and negatives labels in the target is preserved in each repetition. \n\nWe have chosen to use the area under the ROC curve as our scoring metric as it is robust to class imbalance, which exists in our target."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n\ncv_method = RepeatedStratifiedKFold(n_splits=5, \n                                    n_repeats=3, \n                                    random_state=random_seed)\n\nscoring_metric = 'roc_auc'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"feature-sel\"></a>\n## Feature Selection\nWe will optimise our dataset by peforming feature selection. This will be done using random forest importance (RFI) to determine whether an optimal subset of our descriptive features might give better performance. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# for plotting\nimport altair as alt\nalt.renderers.enable('notebook')\n\nfrom sklearn import feature_selection as fs\nfrom sklearn.ensemble import RandomForestClassifier\n\n# intially we wish to asses the importance of all of our features\nnum_features = 20\n\nmodel_rfi = RandomForestClassifier(n_estimators=200)\nmodel_rfi.fit(data, encoded_target)\nfs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:num_features]\n\n# get all features in sorted order of importance\nbest_features_rfi = data_copy.columns[fs_indices_rfi].values\n\n# get their importances\nfeature_importances_rfi = model_rfi.feature_importances_[fs_indices_rfi]\n\n# plot\ndf_fs = pd.DataFrame({'features': best_features_rfi, \n                      'importances' : feature_importances_rfi})\n    \nalt.Chart(df_fs, \n            width=800,\n            height=500,\n            title='Random Forest Feature Importances',\n            ).mark_bar(opacity=0.75, \n                        color='blue').encode(\n    alt.X('features', title='features', sort=None, axis=alt.AxisConfig(labelAngle=45)),\n    alt.Y('importances', title='importances')\n    )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the above graph we can see the importances of all of our descriptive features sorted in descreasing order of importance. Let us now measure the accuracy of a number of feature subsets to determine if one is optimal using a KNN classifier with the eulidean distance metric and n=5 neighbours."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.neighbors import KNeighborsClassifier\n\n# our classifier for measuring purposes\nfs_knn = KNeighborsClassifier(n_neighbors=5)\n\n# minimum of 5 features, maximum of all of them\nnum_features = [x for x in range(5,21)]\n\n# record of scores, preserving the order such that i-th entry of num_features corresponds to the i-th entry of fs_scores\nfs_scores = list()\n\nfor n in num_features:\n    \n    # subset data into the first n most important features\n    subset = data[:, fs_indices_rfi[:n]]\n    \n    # assess accuracy and record it\n    cv_results_fs = cross_val_score(estimator=fs_knn,\n                             X=subset,\n                             y=encoded_target, \n                             cv=cv_method, \n                             scoring=scoring_metric)\n\n    fs_scores.append(cv_results_fs.mean().round(3))\n    \n# df of scores in sorted order\nacc_df = pd.DataFrame({'num_features': num_features, \n                       'score': fs_scores}).sort_values(by=['score']) \n\n# get row containing optimal n_features\nn_features_optimal = acc_df.iloc[acc_df['score'].argmax()]\n\n# display df\nacc_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# plot results\nalt.Chart(acc_df, \n          width=500,\n          height=700,\n          title='scores for the n most important features'\n         ).mark_line(point=True).encode(\n    alt.X('num_features', title='num_features'),\n    alt.Y('score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Above we see that using the 12 most important features yields us the greatest mean CV score. We therefore subset our data accordingly before proceeding."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# obtain the value for the optimal number of features\nbest_n = int(n_features_optimal['num_features'])\n# transform the data to include just these features\ndata = data[:, fs_indices_rfi[:best_n]]\ndata.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"hyper-tun\"></a>\n## Hyperparameter Tuning\nTo optimise our models we will perform hyperparameter tuning. The models we will be optimising are: K-nearest neighbours, decision tree, random forest, and naive bayes. Grid search is used to determine optimal parameters and models are trained on the training dataset."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Train-test Splitting\nWe require that our data be split into training and testing sets. Note that to preserve the proportion of positive and negative instances of our target across the training and test sets we set the stratify option to the target."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nd_train, d_test,t_train, t_test = train_test_split(data, encoded_target, \n                                                    test_size = 0.3, \n                                                    stratify = encoded_target,   \n                                                    random_state=random_seed)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### KNN hyperparameter tuning\nWe begin by defining a set of parameters for tuning our KNN model. We wish to inspect as large a range as possible without incurring too much computational cost."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\n# parameters for KNN tuning\nKNN_params = {'n_neighbors' : [x for x in range(1,20)],\n             'p' : [1,2,3,4,5]}\n\n# grid search setup\ngs_KNN = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=KNN_params, \n                      cv=cv_method,\n                      verbose=1,\n                      n_jobs=-1,\n                      scoring=scoring_metric)\n\n# fit the KNN model using training data\ngs_KNN.fit(d_train, t_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have our optimal parameters, let's visualise the results of our hyperparamter tuning and cross-validation. "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# print results\nprint(f\"The optimal KNN parameters determined by grid search are:\\n{gs_KNN.best_params_}\\n\")\nprint(f\"These parameters give the best score of:{round(gs_KNN.best_score_,3)}\\n\")\n\n# we build a dataframe to hold the results of our tuning process so that we may plot them\nresults_KNN = pd.DataFrame(gs_KNN.cv_results_['params'])\nresults_KNN['test_score'] = gs_KNN.cv_results_['mean_test_score']\nresults_KNN['metric'] = results_KNN['p'].replace([1,2,3,4,5], [\"Manhattan\", \"Euclidean\", \"Minkowski3\", \"Minkowski4\", \"Minkowski5\"])\n\n# plot\nalt.Chart(results_KNN, \n          width=1000,\n          height=500,\n          title='KNN Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('n_neighbors', title='Number of Neighbors'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='metric'\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From the plot above it's clear to see that increasing the value of p does not improve performance, however at the end of our range of values for n_neighbours it seems as if our accuracy score might increase more. \n\nWe therefore extend our range of values for n_neighbors, reducing our values of p to save on computational time, to see the effect of raising this value. We will check only odd values to save time and to guard against tied-votes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# parameters for KNN tuning\nKNN_params_further = {'n_neighbors' : [x for x in range(1,80,2)],\n             'p' : [1,2]}\n\n# grid search setup\ngs_KNN_further = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=KNN_params_further, \n                      cv=cv_method,\n                      verbose=1,\n                      n_jobs=-1,\n                      scoring=scoring_metric)\n\n# fit the KNN model using training data\ngs_KNN_further.fit(d_train, t_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# print results\nprint(f\"The optimal KNN parameters determined by grid search are:\\n{gs_KNN_further.best_params_}\\n\")\nprint(f\"These parameters give the best score of:{round(gs_KNN_further.best_score_,3)}\\n\")\n\n# we build a dataframe to hold the results of our tuning process so that we may plot them\nresults_KNN_further = pd.DataFrame(gs_KNN_further.cv_results_['params'])\nresults_KNN_further['test_score'] = gs_KNN_further.cv_results_['mean_test_score']\nresults_KNN_further['metric'] = results_KNN_further['p'].replace([1,2], [\"Manhattan\", \"Euclidean\"])\n\n# plot\nalt.Chart(results_KNN_further, \n          width=1000,\n          height=500,\n          title='KNN Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('n_neighbors', title='Number of Neighbors'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='metric'\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From the plot of our extended range of values it's clear to see that raising the value of n_neighbors does not significantly increase our mean CV score, with there being virtually no difference between n=9 with a score of 0.91 and n=31 with a score of 0.912. We therefore keep our optimal paramter at n=9."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n### Decision Tree\nWe proceed as before with our decision tree classifier to determine the optimal parameters. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\n\nparams_DT = {'criterion': ['gini', 'entropy'],\n             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n             'min_samples_split': [2, 4, 6, 8, 10]}\n\ngs_DT = GridSearchCV(estimator=DecisionTreeClassifier(random_state=random_seed), \n                     param_grid=params_DT, \n                     cv=cv_method,\n                     verbose=1, \n                     n_jobs=-1,\n                     scoring=scoring_metric)\n\ngs_DT.fit(d_train, t_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The plot below shows that the entropy metric appears to outperform the gini metric for this model, with the two becoming approximately the same as max_depth increases and overall performance decreases."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# display results\nprint(f\"The optimal DT parameters determined by grid search are:\\n{gs_DT.best_params_}\\n\")\nprint(f\"These parameters give the best score of:{gs_DT.best_score_}\\n\")\n\n# record the results in a dataframe for plotting\nresults_DT = pd.DataFrame(gs_DT.cv_results_['params'])\nresults_DT['test_score'] = gs_DT.cv_results_['mean_test_score']\n\n# plot\nalt.Chart(results_DT, \n          width = 1000,\n          height=500,\n          title='DT Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('max_depth', title='Maximum Depth'),\n    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n    color='criterion:N'\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's also visualise our results in terms of our minimal sample split parameters. We see fairly uniform performance until a max_depth of 3, after which performance for each value diverges and is suboptimal."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "alt.Chart(results_DT, \n          width = 1000,\n          height=500, \n          title='DT Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('max_depth', title='Maximum Depth'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='min_samples_split:N' \n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Naive Bayes"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "One of the assumptions of a Naive Bayes learner is that our descriptive features are at least approximately normally distributed. To normalise our descriptive features for this model we will apply a power transformer to our training data before tuning. This is done below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import PowerTransformer\n\n# copy the data\nNB_d_train = d_train.copy()\n\n# apply the power transformer to the data\nNB_d_train = PowerTransformer().fit_transform(NB_d_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We're now ready to tune our Naive Bayes learner."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\n\nparams_NB = {'var_smoothing': np.logspace(1,-11, num=300)}\n\ngs_NB = GridSearchCV(estimator=GaussianNB(), \n                     param_grid=params_NB, \n                     cv=cv_method,\n                     verbose=1, \n                     n_jobs=-1,\n                     scoring=scoring_metric)\n\ngs_NB.fit(NB_d_train, t_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can report our optimal parameters and visualise our results. From the plot below we see that as values of var_smoothing increase performance decreases."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# display results\nprint(f\"The optimal NB parameters determined by grid search are:\\n{gs_NB.best_params_}\\n\")\nprint(f\"These parameters give the best score of:{round(gs_NB.best_score_,3)}\\n\")\n\n# create a dataframe for plotting our results\nresults_NB = pd.DataFrame(gs_NB.cv_results_['params'])\nresults_NB['test_score'] = gs_NB.cv_results_['mean_test_score']\n\n# plot\nalt.Chart(results_NB, \n          width = 1000,\n          height=500,\n          title='NB Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('var_smoothing', title='Var. Smoothing'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False))\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Random Forest\nLastly we will be tuning a random forest learner. We proceed as before. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# parameters\nRF_params = {'n_estimators': [50, 100, 200, 300, 400 ,500, 600],\n             'max_depth': [2,4,8,16]} \n\ngs_RF = GridSearchCV(estimator=RandomForestClassifier(random_state=random_seed), \n                     param_grid=RF_params, \n                     cv=cv_method,\n                     verbose=1,\n                     n_jobs=-1,\n                     scoring=scoring_metric)\n\ngs_RF.fit(d_train, t_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# display results\nprint(f\"The optimal RF parameters determined by grid search are:\\n{gs_RF.best_params_}\\n\")\nprint(f\"These parameters give the best score of:{gs_RF.best_score_}\\n\")\n\n# create a dataframe for plotting our results\nresults_RF = pd.DataFrame(gs_RF.cv_results_['params'])\nresults_RF['test_score'] = gs_RF.cv_results_['mean_test_score']\n\n# TODO, can squeeze more performance out of this, do little investigation then bigger optimised one\n\n# plot\nalt.Chart(results_RF, \n          width = 1000,\n          height=500, \n          title='RF Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('n_estimators', title='n_estimators'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='max_depth:N'\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"model-fit\"></a>\n# Model Fitting and Performance Evaluation\nNow that we have the optimal parameters for our models by training them on the training data, we can not fit them to our test data and evaluate their performance. The aim of this is to evaluate the performance of our learners on data that they have not yet seen, and to then comapre their performance using paired t-tests to determine whether the differences in performance are significant."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## KNN evaluation\nWe proceed by fitting a KNN learner (with optimised paramters) to our testing data set and evaulating its performance using our chosen cross-validation scheme. We do the same for the other 3 learners below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "KNN_eval = KNeighborsClassifier(n_neighbors=gs_KNN.best_params_['n_neighbors'],\n                                p=gs_KNN.best_params_['p'])\n\ncv_results_KNN = cross_val_score(estimator=KNN_eval,\n                                 X=d_test,\n                                 y=t_test, \n                                 cv=cv_method, \n                                 n_jobs=-1,\n                                 scoring=scoring_metric)\ncv_results_KNN.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Decision Tree evaluation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "DT_eval = DecisionTreeClassifier(criterion=gs_DT.best_params_['criterion'],\n                                 max_depth=gs_DT.best_params_['max_depth'],\n                                 min_samples_split=gs_DT.best_params_['min_samples_split'],\n                                 random_state=random_seed)\n\ncv_results_DT = cross_val_score(estimator=DT_eval,\n                                X=d_test,\n                                y=t_test, \n                                cv=cv_method, \n                                n_jobs=-1,\n                                scoring=scoring_metric)\ncv_results_DT.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Naive Bayes evaluation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "NB_eval = GaussianNB(var_smoothing=gs_NB.best_params_['var_smoothing'])\n\nd_test_transformed = PowerTransformer().fit_transform(d_test)\n\ncv_results_NB = cross_val_score(estimator=NB_eval,\n                                X=d_test_transformed,\n                                y=t_test, \n                                cv=cv_method, \n                                n_jobs=-1,\n                                scoring=scoring_metric)\ncv_results_NB.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Random Forest evaluation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "RF_eval = RandomForestClassifier(n_estimators=gs_RF.best_params_['n_estimators'],\n                                 max_depth=gs_RF.best_params_['max_depth'],\n                                 random_state=random_seed)\n\ncv_results_RF = cross_val_score(estimator=RF_eval,\n                                X=d_test,\n                                y=t_test, \n                                cv=cv_method, \n                                n_jobs=-1,\n                                scoring=scoring_metric)\ncv_results_RF.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Paired t-tests of model performace "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy import stats\n\nprint(\"KNN vs NB:\",stats.ttest_rel(cv_results_KNN, cv_results_NB))\nprint(\"KNN vs DT:\",stats.ttest_rel(cv_results_KNN, cv_results_DT))\nprint(\"KNN vs RF:\",stats.ttest_rel(cv_results_KNN, cv_results_RF))\nprint(\"RF vs NB:\",stats.ttest_rel(cv_results_RF, cv_results_NB))\nprint(\"DT vs NB:\",stats.ttest_rel(cv_results_DT, cv_results_NB))\nprint(\"RF vs DT:\",stats.ttest_rel(cv_results_RF, cv_results_DT))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "p < 0.05 indicates a statistically significant difference in performance at a 95% confidence level. All differences are significant except for the Naive Bayes model compared to the Random Forest model, which were the two best learners with a small difference between their scores. \n\nTo try and remedy this, we will re-evaluate both these models using the entire dataset to see if a significant difference arises."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# re-evaluating the NB model\ndata_NB_transformed = PowerTransformer().fit_transform(data)\n\ncv_results_NB_again = cross_val_score(estimator=NB_eval,\n                                X=data_NB_transformed,\n                                y=target, \n                                cv=cv_method, \n                                n_jobs=-1,\n                                scoring=scoring_metric)\ncv_results_NB_again.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# re-evaluating the RF model\ncv_results_RF_again = cross_val_score(estimator=RF_eval,\n                                X=data,\n                                y=target, \n                                cv=cv_method, \n                                n_jobs=-1,\n                                scoring=scoring_metric)\ncv_results_RF_again.mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(stats.ttest_rel(cv_results_RF_again, cv_results_NB_again))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here we find p < 0.05, and the Naive Bayes model is the clear winner. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"discussion\"></a>\n# Discussion"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using the metrics module from Sci-Kit Learn we can generate the predicitons made by our models on the testing data and examine a number of extra performance metrics."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import metrics\n\npred_KNN = gs_KNN.predict(d_test)\npred_NB = gs_NB.predict(d_test_transformed)\npred_DT = gs_DT.predict(d_test)\npred_RF = gs_RF.predict(d_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"conf-mat\"></a>\n## Confusion Matrix, precision, recall and F1-score\nA confusion matrix will be produced to analyse the outcome of the predictions further. The metrics precision, recall (True Positive Rate) and F1-score are tools to value the results on predictions being made on the test data. These are displayed from the classification_report below for each target.\n\n\\#TODO: Check which target is what, makes a big difference on TPR/recall "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"\\nClassification report for K-Nearest Neighbor\") \nprint(metrics.classification_report(t_test, pred_KNN))\nprint(\"\\nClassification report for Decision Tree\") \nprint(metrics.classification_report(t_test, pred_DT))\nprint(\"\\nClassification report for Naive Bayes\") \nprint(metrics.classification_report(t_test, pred_NB))\nprint(\"\\nClassification report for Random Forest\") \nprint(metrics.classification_report(t_test, pred_RF))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Confusion Matrix are built up in a matrix where each item is\n- $M_{0,0}$  = True negatives (TN)\n- $M_{1,0}$  = False negative (FN)\n- $M_{1,1}$  = True positives (TP)\n- $M_{0,1}$  = False positives (FP)\n\nAs seen in the array below the Confusion Matrix will be shown with values on each positive and negative in that structure."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.array([['TN',   'FP'], ['FN',  'TP']])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"\\nConfusion matrix for K-Nearest Neighbor\") \nprint(metrics.confusion_matrix(t_test, pred_KNN))\nprint(\"\\nConfusion matrix for Decision Tree\") \nprint(metrics.confusion_matrix(t_test, pred_DT))\nprint(\"\\nConfusion matrix for Naive Bayes\") \nprint(metrics.confusion_matrix(t_test, pred_NB))\nprint(\"\\nConfusion matrix for Random Forest\") \nprint(metrics.confusion_matrix(t_test, pred_RF))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"roc-curve\"></a>\n## ROC Curve\nThe ROC curve is a measurement of different predicitors outcome of the True Positive Rate on the False Positive rate, especially good for analysing Binary Classification problems. The area under the ROC curve (AUC) and the F-score are metrics to show how well the predictor is performing."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Preparing the data for the ROC curve."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "t_prob_KNN = gs_KNN.predict_proba(d_test)\nt_prob_DT = gs_DT.predict_proba(d_test)\nt_prob_NB = gs_NB.predict_proba(d_test)\nt_prob_RF = gs_RF.predict_proba(d_test)\nfpr_KNN, tpr_KNN, _ = metrics.roc_curve(t_test, t_prob_KNN[:, 1])\nfpr_DT, tpr_DT, _ = metrics.roc_curve(t_test, t_prob_DT[:, 1])\nfpr_NB, tpr_NB, _ = metrics.roc_curve(t_test, t_prob_NB[:, 1])\nfpr_RF, tpr_RF, _ = metrics.roc_curve(t_test, t_prob_RF[:, 1])\n\nprint(\"AUC for KNN: {}\".format(round(metrics.auc(fpr_KNN, tpr_KNN), 3)))\nprint(\"AUC for Decision Tree: {}\".format(round(metrics.auc(fpr_DT, tpr_DT), 3)))\nprint(\"AUC for Naive Bayes: {}\".format(round(metrics.auc(fpr_NB, tpr_NB), 3)))\nprint(\"AUC for Random Forest: {}\".format(round(metrics.auc(fpr_RF, tpr_RF), 3)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\\#TODO: KNN and RF best AUC but very little difference"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A dataframe is created for False Positive Rate and True Positive Rate below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_KNN = pd.DataFrame({'fpr': fpr_KNN, 'tpr': tpr_KNN})\ndf_DT = pd.DataFrame({'fpr': fpr_DT, 'tpr': tpr_DT})\ndf_NB = pd.DataFrame({'fpr': fpr_NB, 'tpr': tpr_NB})\ndf_RF = pd.DataFrame({'fpr': fpr_RF, 'tpr': tpr_RF})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Visualizing the ROC curve for each method using Altair."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base = alt.Chart(df_KNN, \n                 title='ROC Curve of KNN'\n                ).properties(width=300)\n\nroc_curve = base.mark_line(point=True).encode(\n    alt.X('fpr', title='False Positive Rate (FPR)',  sort=None),\n    alt.Y('tpr', title='True Positive Rate (TPR) (a.k.a Recall)'),\n)\n\nroc_rule = base.mark_line(color='green').encode(\n    x='fpr',\n    y='fpr',\n    size=alt.value(2)\n)\n\n(roc_curve + roc_rule).interactive()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base = alt.Chart(df_DT, \n                 title='ROC Curve of Decision Tree'\n                ).properties(width=300)\n\nroc_curve = base.mark_line(point=True).encode(\n    alt.X('fpr', title='False Positive Rate (FPR)',  sort=None),\n    alt.Y('tpr', title='True Positive Rate (TPR) (a.k.a Recall)'),\n)\n\nroc_rule = base.mark_line(color='green').encode(\n    x='fpr',\n    y='fpr',\n    size=alt.value(2)\n)\n\n(roc_curve + roc_rule).interactive()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base = alt.Chart(df_NB, \n                 title='ROC Curve of Naive Bayes'\n                ).properties(width=300)\n\nroc_curve = base.mark_line(point=True).encode(\n    alt.X('fpr', title='False Positive Rate (FPR)',  sort=None),\n    alt.Y('tpr', title='True Positive Rate (TPR) (a.k.a Recall)'),\n)\n\nroc_rule = base.mark_line(color='green').encode(\n    x='fpr',\n    y='fpr',\n    size=alt.value(2)\n)\n\n(roc_curve + roc_rule).interactive()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "base = alt.Chart(df_RF, \n                 title='ROC Curve of Random Forest'\n                ).properties(width=300)\n\nroc_curve = base.mark_line(point=True).encode(\n    alt.X('fpr', title='False Positive Rate (FPR)',  sort=None),\n    alt.Y('tpr', title='True Positive Rate (TPR) (a.k.a Recall)'),\n)\n\nroc_rule = base.mark_line(color='green').encode(\n    x='fpr',\n    y='fpr',\n    size=alt.value(2)\n)\n\n(roc_curve + roc_rule).interactive()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\\# TODO: Analysing where the \"elbow\", the curve bends, towards TPR=1.\nThe model indicates that it has a good///bad performance.\n#TODO: also need to talk about what all these measurements tells us about the models"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#TODO: Here I will talk about the limitations etc\n\nNow we turn to a discussion of some of the strengths and weaknesses of our study.\n\nFirstly, while our dataset was complete and fairly clean, it was quite small with only 270 observations. While we lack the expertise to comment on the number and nature of appropriate descriptive features for heart disease we feel safe to say that having a much larger number of observations, say 5000 or so, would increase our confidence in our results. However, despite having\na small dataset we feel our learners performed very well, with their average performance on the testing data given below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_scores = [cv_results_NB.mean(),cv_results_KNN.mean(),cv_results_DT.mean(),cv_results_RF.mean()]\navg_test_score = sum(test_scores)/len(test_scores)\nprint(\"The average mean CV score for all models is \",round(avg_test_score,3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This study could be further improved by examining more machine learning models such as support vector machines and neural networks as it remains to be seen how these models perform, however due to time constraints we were not able to measure them. Computational resources were another limitation, as not wanting to incur too much computational cost we were perhaps restricted in the range of values we could measure as well as the type of cross-validation method we chose. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"conclusion\"></a>\n# Conclusion"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our investigation demonstrates that machine learning can be an important tool in diagnostics, with all models examined showing strong predictive power. What we've also seen is that the performance of a machine learning model can be very sensitive to noisy data as well as the exact parameters of the model itself. To address both of these sensitivities we performed feature selection and hyperparameter tuning in a methodical way to maximise performance as much as possible given time and computational constraints. \n\nOur results show that the performance of the Naive Bayes model was greater than that of the other models, and that this difference in performance was statistically significant. We have therefore achieved our original aim. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"refs\"></a>\n# References"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "UCI Machine Learning Repository. (n.d.). *Statlog (Heart) Data Set*. https://archive.ics.uci.edu/ml/datasets/statlog+%28heart%29"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}