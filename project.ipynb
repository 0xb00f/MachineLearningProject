{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Machine Learning Project"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Introduction"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The aim of this project is to predict the presence of heart disease using a dataset obtained from the UCL Machine Learning repository. The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/statlog+%28heart%29). \n\n*INSERT OUTLINE OF METHODOLOGY AFTER PROJECT*"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Description"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our data consists of 270 instances with 13 descriptive features, and a target feature having two classes which indicate the presence or absence of heart disease. Hence, this is a binary classification problem. As is, our dataset contains no missing values.\n\nThe following explanation of the descriptive features is extracted from the dataset description on the [UCL website](https://archive.ics.uci.edu/ml/datasets/statlog+%28heart%29):\n* age - **numerical**\n* sex - **binary**\n* chest pain type - **nominal**\n* resting blood pressure - **numerical**\n* serum cholestoral in mg/dl - **numerical**      \n* fasting blood sugar > 120 mg/dl - **binary**\n* resting electrocardiographic results - **nominal**\n* maximum heart rate achieved - **numerical**\n* exercise induced angina - **binary**\n* oldpeak = ST depression induced by exercise relative to rest - **numerical**\n* the slope of the peak exercise ST segment - **ordinal**\n* number of major vessels (0-3) colored by flourosopy - **numerical**    \n* thal - **nominal**\n\nEach of the nominal features has been integer encoded, which is a miselading representation of the data and will be addressed in the processing stage. \n\nTime to import our dataset as well as any modules we will be using, and do some preliminary configuration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Ignore python warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# display all columns\npd.set_option('display.max_columns', None) \n\n# read in and configure data with column names\n# col_names = ['age', 'sex', 'chest_pain_type', 'resting_bp', 'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results', 'max_hr_achieved', 'exercise_induced_angina', 'oldpeak', 'slope_of_peak_exercise', 'no_of_major_vessels', 'thal', 'target']\n# these names are abbreviated below for space\ncol_names = ['age', 'sex', 'cpt', 'rb', 'sc', 'fbs', 'rer', 'mha', 'eia', 'old', 'sope', 'nomv', 'thal', 'target'] \ndata = pd.read_csv('heart.csv',names=col_names,header=None)\n\n# separate target feature\ntarget = data['target']\ndata.drop(columns=['target'],inplace=True)",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have our dataset imported and our target feature separated we can have a brief look at our dataset using python."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"----------\")\nprint(f\"Shape of the dataset is {data.shape} \\n\")\nprint(\"----------\")\nprint(f\"Each of the descriptive features have the following types:\\n{data.dtypes}\\n\")\nprint(\"----------\")\nprint(f\"Each of the descriptive features have the following number of unique values:\\n{data.nunique()}\\n\")\nprint(\"----------\")\nprint(f\"The dataset contains no missing values:\\n{data.isna().sum()}\\n\")",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "----------\nShape of the dataset is (270, 13) \n\n----------\nEach of the descriptive features have the following types:\nage     float64\nsex     float64\ncpt     float64\nrb      float64\nsc      float64\nfbs     float64\nrer     float64\nmha     float64\neia     float64\nold     float64\nsope    float64\nnomv    float64\nthal    float64\ndtype: object\n\n----------\nEach of the descriptive features have the following number of unique values:\nage      41\nsex       2\ncpt       4\nrb       47\nsc      144\nfbs       2\nrer       3\nmha      90\neia       2\nold      39\nsope      3\nnomv      4\nthal      3\ndtype: int64\n\n----------\nThe dataset contains no missing values:\nage     0\nsex     0\ncpt     0\nrb      0\nsc      0\nfbs     0\nrer     0\nmha     0\neia     0\nold     0\nsope    0\nnomv    0\nthal    0\ndtype: int64\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Preparation\n<a id='dataprep'></a>\nWhile our dataset comes to us fairly clean, there is still much data preparation to do before we can get to the business of predictive modelling.\n\nFirst of all, we must integer encode the target feature. It is holds the values (1) for the absence of heart disease, and (2) for the presence of heart disease. Since the presence is our postive feature, we wish to map this to (1) and the absence to (0)."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# firstly we integer encode target feature \nprint(\"Target before encoding: \",np.unique(target,return_counts=True))\nencoded_target = np.where(target==1,0,1)\nprint(\"Target after encoding: \",np.unique(encoded_target,return_counts=True))",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Target before encoding:  (array([1, 2]), array([150, 120]))\nTarget after encoding:  (array([0, 1]), array([150, 120]))\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now turn to our nominal features (chest pain type, resting electrocardiographic results, and thal) which have been integer encoded. As integer encoding assumes an ordering, we consider it bad practice to integer encode nominal features, so we will undo this encoding to repace it with a one-hot encoding scheme."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# output the value counts for variable 'cpt'\nprint(f\"Before processing the variable \\\"cpt\\\" has the following value counts:\\n{data['cpt'].value_counts()}\\n\")\n\n# we then map each of them to the following categorical levels\ncpt_mappings = {1.0 : 'cpt1', 2.0 : 'cpt2', 3.0 : 'cpt3', 4.0 : 'cpt4'}\ndata['cpt'].replace(cpt_mappings,inplace=True)\n\n# output the result\nprint(f\"After processing the variable \\\"cpt\\\" has the following value counts:\\n{data['cpt'].value_counts()}\\n\")\n\n# output the value counts for variable 'rer'\nprint(f\"Before processing the variable \\\"rer\\\" has the following value counts:\\n{data['rer'].value_counts()}\\n\")\n\n# we then map each of them to the following categorical levels\nrer_mappings = {0.0 : 'rer0', 1.0 : 'rer1', 2.0 : 'rer2'}\ndata['rer'].replace(rer_mappings,inplace=True)\n\n# output the result\nprint(f\"After processing the variable \\\"rer\\\" has the following value counts:\\n{data['rer'].value_counts()}\\n\")\n\n# output the value counts for variable 'thal'\nprint(f\"Before processing the variable \\\"thal\\\" has the following value counts:\\n{data['thal'].value_counts()}\\n\")\n\n# we then map each of them to the following categorical levels\nthal_mappings = {3.0 : 'thal3', 6.0 : 'thal6', 7.0 : 'thal7'}\ndata['thal'].replace(thal_mappings,inplace=True)\n\n# output the result\nprint(f\"After processing the variable \\\"thal\\\" has the following value counts:\\n{data['thal'].value_counts()}\\n\")",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Before processing the variable \"chest_pain_type\" or \"cpt\" has the following value counts:\n4.0    129\n3.0     79\n2.0     42\n1.0     20\nName: cpt, dtype: int64\n\nAfter processing the variable \"cpt\" has the following value counts:\ncpt4    129\ncpt3     79\ncpt2     42\ncpt1     20\nName: cpt, dtype: int64\n\nBefore processing the variable \"rer\" has the following value counts:\n2.0    137\n0.0    131\n1.0      2\nName: rer, dtype: int64\n\nAfter processing the variable \"rer\" has the following value counts:\nrer2    137\nrer0    131\nrer1      2\nName: rer, dtype: int64\n\nBefore processing the variable \"thal\" has the following value counts:\n3.0    152\n7.0    104\n6.0     14\nName: thal, dtype: int64\n\nAfter processing the variable \"thal\" has the following value counts:\nthal3    152\nthal7    104\nthal6     14\nName: thal, dtype: int64\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we've undone the integer encoding, we're ready to apply one-hot encoding to these nominal variables."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.get_dummies(data)\ndata.head()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>rb</th>\n      <th>sc</th>\n      <th>fbs</th>\n      <th>mha</th>\n      <th>eia</th>\n      <th>old</th>\n      <th>sope</th>\n      <th>nomv</th>\n      <th>cpt_cpt1</th>\n      <th>cpt_cpt2</th>\n      <th>cpt_cpt3</th>\n      <th>cpt_cpt4</th>\n      <th>rer_rer0</th>\n      <th>rer_rer1</th>\n      <th>rer_rer2</th>\n      <th>thal_thal3</th>\n      <th>thal_thal6</th>\n      <th>thal_thal7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>70.0</td>\n      <td>1.0</td>\n      <td>130.0</td>\n      <td>322.0</td>\n      <td>0.0</td>\n      <td>109.0</td>\n      <td>0.0</td>\n      <td>2.4</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>67.0</td>\n      <td>0.0</td>\n      <td>115.0</td>\n      <td>564.0</td>\n      <td>0.0</td>\n      <td>160.0</td>\n      <td>0.0</td>\n      <td>1.6</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>124.0</td>\n      <td>261.0</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64.0</td>\n      <td>1.0</td>\n      <td>128.0</td>\n      <td>263.0</td>\n      <td>0.0</td>\n      <td>105.0</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>74.0</td>\n      <td>0.0</td>\n      <td>120.0</td>\n      <td>269.0</td>\n      <td>0.0</td>\n      <td>121.0</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "    age  sex     rb     sc  fbs    mha  eia  old  sope  nomv  cpt_cpt1  \\\n0  70.0  1.0  130.0  322.0  0.0  109.0  0.0  2.4   2.0   3.0         0   \n1  67.0  0.0  115.0  564.0  0.0  160.0  0.0  1.6   2.0   0.0         0   \n2  57.0  1.0  124.0  261.0  0.0  141.0  0.0  0.3   1.0   0.0         0   \n3  64.0  1.0  128.0  263.0  0.0  105.0  1.0  0.2   2.0   1.0         0   \n4  74.0  0.0  120.0  269.0  0.0  121.0  1.0  0.2   1.0   1.0         0   \n\n   cpt_cpt2  cpt_cpt3  cpt_cpt4  rer_rer0  rer_rer1  rer_rer2  thal_thal3  \\\n0         0         0         1         0         0         1           1   \n1         0         1         0         0         0         1           0   \n2         1         0         0         1         0         0           0   \n3         0         0         1         1         0         0           0   \n4         1         0         0         0         0         1           1   \n\n   thal_thal6  thal_thal7  \n0           0           0  \n1           0           1  \n2           0           1  \n3           0           1  \n4           0           0  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that our descriptive features are encoded correctly we can now scale our data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\n\n# save a copy of the original data\ndata_copy = data.copy()\n# perform scaling\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(data)\ndata = scaler.fit_transform(data)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our data has now been processed and scaled and we're ready to move to WHE"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Exploration & Visualisation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}